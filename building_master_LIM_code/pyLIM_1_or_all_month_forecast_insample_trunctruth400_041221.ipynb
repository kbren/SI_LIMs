{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIM forecasting: multivariate separate sic each month\n",
    "\n",
    "Katie Brennan  \n",
    "started March 2021  \n",
    "  \n",
    "#### Goals: \n",
    "* Build a LIM based on SIC \n",
    "* Build a LIM based on both SIC and SIT \n",
    "* Compare performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import scipy as spy\n",
    "import pickle \n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.util import add_cyclic_point\n",
    "from collections import OrderedDict \n",
    "\n",
    "import time as timestamp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/disk/p/mkb22/Documents/si_analysis_kb/LIMs/SI_LIMs/\")\n",
    "import LIM_utils as lim\n",
    "\n",
    "import LIM_utils_kb as limkb\n",
    "import LIM_stats_kb as statskb\n",
    "import LIM_plot_kb as plotkb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'LIM_stats_kb' from '/home/disk/p/mkb22/Documents/si_analysis_kb/LIMs/SI_LIMs/LIM_stats_kb.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(limkb)\n",
    "importlib.reload(statskb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc_proj = dict(projection=ccrs.Stereographic(central_latitude=90,\n",
    "                                              central_longitude=-45,\n",
    "                                              true_scale_latitude=0.1))\n",
    "proj = dict(projection=ccrs.Robinson(central_longitude=0.),zorder=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of EOFs to retain for the LIM state vector\n",
    "ntrunc = 400 # EOF truncation for individual fields (reduced-state space)\n",
    "nmodes = 60 # number of coupled EOFs for LIM state space (truncated-state space)\n",
    "#nmodes = 25\n",
    "nmodes_sic = 50\n",
    "#modes_sic = 20\n",
    "\n",
    "mo='all'\n",
    "\n",
    "# forecast lead time in months that defines the LIM training\n",
    "tau = 1\n",
    "\n",
    "# training data defined by the first ntrain times\n",
    "# fraction of years used in training\n",
    "# ensures that start year of both train and validation data is january \n",
    "ntrain = 0.6\n",
    "\n",
    "# variables to include in the LIM (note \"vars\" is a Python command)\n",
    "#limvars = ['tas','zg']\n",
    "#limvars = ['tas','rlut','zg']\n",
    "#limvars = ['sic']\n",
    "#limvars = ['tas','sic']\n",
    "#limvars = ['tas','sic','zg','psl','pr','tos']\n",
    "#limvars = ['tas','psl','tos','sit','sic']\n",
    "# limvars = ['tas','tos','psl','sit','sic']\n",
    "# limvars_nosic = ['tas','tos','psl','sit']\n",
    "limvars = ['tas','tos','sic']\n",
    "limvars_nosic = ['tas','tos']\n",
    "nvars = len(limvars)\n",
    "\n",
    "# specify the model source \n",
    "# train_dsource = 'mpi_lm_kb'\n",
    "train_dsource = 'ccsm4_lm_kb'\n",
    "# valid_dsource = 'mpi_lm_kb'\n",
    "valid_dsource = 'ccsm4_lm_kb'\n",
    "\n",
    "sic_separate = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_names = {'tas':'Surface air temperature',\n",
    "              'psl':'Sea level Pressure',\n",
    "              'sic':'Sea ice concentration', \n",
    "              'sit':'Sea ice thickness',\n",
    "              'tos':'Sea surface temperature',\n",
    "              'zg': '500hPa geopotential height'}\n",
    "\n",
    "areawt_name = {'tas':'areacella',\n",
    "               'psl':'areacella',\n",
    "               'sic':'areacello', \n",
    "               'sit':'areacello',\n",
    "               'tos':'areacello',\n",
    "               'zg': 'areacella'}\n",
    "\n",
    "month_names = ['January','Februrary','March','April','May','June','July','August',\n",
    "               'September','October','November','December']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "today = date.today()\n",
    "\n",
    "#Year-month-day\n",
    "today_date = today.strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = np.pi\n",
    "\n",
    "# fill continents if plotting SST; otherwise no\n",
    "# if var_to_extract == 'tos':\n",
    "#     noland = True\n",
    "# else:\n",
    "#     noland = False\n",
    "\n",
    "infile_20cr_tas = '/home/disk/kalman3/rtardif/LMR/data/model/20cr/tas_sfc_Amon_20CR_185101-201112.nc'\n",
    "\n",
    "fdic_ccsm4 = limkb.build_training_dic(train_dsource)\n",
    "fdic_mpi = limkb.build_training_dic(valid_dsource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areacell = {}\n",
    "areacella_dict = {}\n",
    "areacell['areacella'], areacella_dict = limkb.load_data('areacella', areacella_dict, fdic_ccsm4, \n",
    "                                                  remove_climo=False, detrend=False, verbose=False)\n",
    "\n",
    "areacello_dict = {}\n",
    "areacell['areacello'], areacello_dict = limkb.load_data('areacello', areacello_dict, fdic_ccsm4, \n",
    "                                                  remove_climo=False, detrend=False, verbose=False)\n",
    "\n",
    "areacell_dict = {}\n",
    "areacell_dict['areacello'] = areacello_dict\n",
    "areacell_dict['areacella'] = areacella_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compress full fields: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sic\n",
      "removing climotology...\n",
      "detrending...\n",
      "(122880, 12012)\n",
      "(122880, 12012)\n",
      "-----------------------------------------------------\n",
      "completed in 330.3807327747345 seconds\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# load training data...\n",
    "wt=True\n",
    "limvars = ['sic']\n",
    "#ntrunc_full = 400\n",
    "nmodes_sic = 400\n",
    "var_dict = {}\n",
    "\n",
    "for k, var in enumerate(limvars): \n",
    "    X_var, var_dict = limkb.load_data(var, var_dict, fdic_ccsm4, remove_climo=True, \n",
    "                                      detrend=True, verbose=True)\n",
    "    \n",
    "    tsamp = X_var.shape[1]\n",
    "    \n",
    "    acell = areacell[areawt_name[var]]\n",
    "    if len(acell.shape)>1:\n",
    "        acell_1d = np.reshape(acell,(acell.shape[0]*acell.shape[1]))\n",
    "    else: \n",
    "        acell_1d = acell\n",
    "     \n",
    "    [Ptrunc, E3, tot_var,\n",
    "     tot_var_eig, W_all, \n",
    "     standard_factor] = limkb.step1_compress_individual_var(X_var, var, ntrunc, nmodes_sic, \n",
    "                                                            var_dict, areawt=acell_1d,\n",
    "                                                            wt=wt, sic_separate=False)\n",
    "\n",
    "    var_save = {}\n",
    "    var_save['var_dict'] = var_dict\n",
    "    var_save['Ptrunc'] = Ptrunc\n",
    "    var_save['E3'] = E3\n",
    "    var_save['standard_factor'] = standard_factor\n",
    "    var_save['W_all'] = W_all\n",
    "    \n",
    "    if var =='sic':\n",
    "        savename = (str(var)+'_ntrunc'+str(nmodes_sic)+'_'+valid_dsource+'_'+today_date+'.pkl')\n",
    "    else: \n",
    "        savename = (str(var)+'_ntrunc'+str(ntrunc)+'_'+valid_dsource+'_'+today_date+'.pkl')\n",
    "    pickle.dump(var_save, open(savename, \"wb\" ) )\n",
    "        \n",
    "    del X_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load compressed raw data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_folder = 'truncated_model_data/'\n",
    "mod_filename = '_ntrunc400_mpi_lm_kb_20210406.pkl'\n",
    "mod_sic_filename = '_ntrunc50_mpi_lm_kb_20210406.pkl'\n",
    "\n",
    "def load_truncated_data(var, mod_folder, mod_filename):\n",
    "    print('Loading truncated '+var)\n",
    "    mod_data = pickle.load(open(mod_folder+var+mod_filename, \"rb\" ) )\n",
    "\n",
    "    var_dict = mod_data['var_dict']\n",
    "    X_var_trunc = mod_data['Ptrunc']\n",
    "    X_var_E3 = mod_data['E3']\n",
    "    X_var_standard_factor = mod_data['standard_factor']\n",
    "    X_var_W_all= mod_data['W_all']\n",
    "    \n",
    "    return X_var_trunc, var_dict, X_var_E3, X_var_standard_factor, X_var_W_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build L from scratch: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data...\n",
    "load = 'truncated'\n",
    "out_sample_valid = True \n",
    "\n",
    "wt=True\n",
    "var_dict = {}\n",
    "tot_var = {}\n",
    "tot_var_eig = {}\n",
    "W_all = {}\n",
    "E3 = {}\n",
    "Ptrunc = {}\n",
    "standard_factor = {}\n",
    "\n",
    "tot_var_valid = {}\n",
    "tot_var_eig_valid = {}\n",
    "W_all_valid = {}\n",
    "E3_valid = {}\n",
    "Ptrunc_valid = {}\n",
    "standard_factor_valid = {}\n",
    "\n",
    "# X_var_E3 = {}\n",
    "# X_var_standard_factor = {}\n",
    "# X_var_W_all = {}\n",
    "\n",
    "#n=0\n",
    "\n",
    "for k, var in enumerate(limvars): \n",
    "    \n",
    "    if load == 'full':\n",
    "        X_var, var_dict = limkb.load_data(var, var_dict, fdic_ccsm4, remove_climo=True, \n",
    "                                          detrend=True, verbose=True)\n",
    "    elif load == 'truncated':\n",
    "        if var == 'sic': \n",
    "            mod_fname = mod_sic_filename\n",
    "        else: \n",
    "            mod_fname = mod_filename\n",
    "    \n",
    "        [X_var, var_dict, E3[var], standard_factor[var], \n",
    "         W_all[var]] = load_truncated_data(var, mod_folder, mod_fname)\n",
    "        print(X_var.shape)\n",
    "\n",
    "#     tsamp = X_var.shape[1]\n",
    "#     nyears_train = int((tsamp*ntrain)/12)\n",
    "#     nyears_valid = int((tsamp*(1-ntrain))/12)\n",
    "\n",
    "#     X_t = np.reshape(X_var,(X_var.shape[0],int(tsamp/12),12))\n",
    "    \n",
    "#     X_train = X_t[:,0:nyears_train,mo:mo+2]\n",
    "#     X_train_2d = np.reshape(X_train,(X_train.shape[0],nyears_train*2))\n",
    "#     ntime = X_train.shape[1]\n",
    "\n",
    "#     X_valid = X_t[:,nyears_train:,mo]\n",
    "    \n",
    "    if mo is 'all':\n",
    "        X_t = X_var\n",
    "        ntime = X_t.shape[1]\n",
    "        nyears_train = 1200\n",
    "        nyears_valid = ntime - nyears_train\n",
    "        \n",
    "        X_train = X_t[:,0:nyears_train]\n",
    "        X_train_2d = X_train\n",
    "        X_valid = X_t[:,nyears_train:]\n",
    "    else: \n",
    "        nyears_train = int((tsamp*ntrain)/12)\n",
    "        nyears_valid = int((tsamp*(1-ntrain))/12)\n",
    "        \n",
    "        X_t = np.reshape(X_var,(X_var.shape[0],int(tsamp/12),12))\n",
    "        \n",
    "        X_train = X_t[:,0:nyears_train,mo:mo+2]\n",
    "        X_train_2d = np.reshape(X_train,(X_train.shape[0],nyears_train*2))\n",
    "        X_valid = X_t[:,nyears_train:,mo]\n",
    "    \n",
    "    acell = areacell[areawt_name[var]]\n",
    "    if len(acell.shape)>1:\n",
    "        acell_1d = np.reshape(acell,(acell.shape[0]*acell.shape[1]))\n",
    "    else: \n",
    "        acell_1d = acell\n",
    "    \n",
    "    if load == 'full':\n",
    "        [Ptrunc[var], E3[var], tot_var[var],\n",
    "         tot_var_eig[var], W_all[var], \n",
    "         standard_factor[var]] = limkb.step1_compress_individual_var(X_train_2d, var, ntrunc, nmodes_sic, \n",
    "                                                                     var_dict, areawt=acell_1d,\n",
    "                                                                     wt=wt, sic_separate=sic_separate)\n",
    "        [Ptrunc_valid[var], E3_valid[var], tot_var_valid[var],\n",
    "         tot_var_eig_valid[var],W_all_valid[var],\n",
    "         standard_factor_valid[var]] = limkb.step1_compress_individual_var(X_valid, var,ntrunc, nmodes_sic, \n",
    "                                                                           var_dict, areawt=acell_1d,wt=wt, \n",
    "                                                                           sic_separate=sic_separate)\n",
    "        \n",
    "    elif load == 'truncated':\n",
    "#         if var == 'sic':\n",
    "#             [Ptrunc[var], E_sic, tot_var[var],\n",
    "#              tot_var_eig[var], W_all_sic, \n",
    "#              standard_factor_sic] = limkb.step1_compress_individual_var(X_train_2d, var, ntrunc, nmodes_sic, \n",
    "#                                                                          var_dict, areawt=acell_1d,\n",
    "#                                                                          wt=False, sic_separate=sic_separate)\n",
    "            \n",
    "#             [Ptrunc_valid[var], E_sic_valid, tot_var_valid[var],\n",
    "#              tot_var_eig_valid[var], W_all_sic_valid, \n",
    "#              standard_factor_sic_valid] = limkb.step1_compress_individual_var(X_valid, var, ntrunc, \n",
    "#                                                                               nmodes_sic, var_dict, \n",
    "#                                                                               areawt=acell_1d, wt=False, \n",
    "#                                                                               sic_separate=sic_separate)\n",
    "#         else: \n",
    "        Ptrunc[var] = X_train_2d \n",
    "        Ptrunc_valid[var] = X_valid\n",
    "\n",
    "    \n",
    "#     [Ptrunc_valid[var], E3_valid[var], tot_var_valid[var],\n",
    "#      tot_var_eig_valid[var],W_all_valid[var],\n",
    "#      standard_factor_valid[var]] = limkb.step1_compress_individual_var(X_valid, var,ntrunc, nmodes_sic, \n",
    "#                                                                        var_dict, areawt=acell_1d,wt=wt, \n",
    "#                                                                        sic_separate=sic_separate)\n",
    "#     for m in range(12):\n",
    "#         X_valid = X_t[:,nyears_train:,m]\n",
    "        \n",
    "#         [Ptrunc_valid, E3_valid, tot_var_valid,\n",
    "#         tot_var_eig_valid, W_all_valid] = step1_compress_individual_var(X_valid, ntrunc, nmodes_sic, var_dict, n, \n",
    "#                                                                         areawt=areacell[areawt_name[var]],\n",
    "#                                                                         wt=wt, sic_separate=sic_separate)\n",
    "#         Ptrunc_valid_var[:,:,m] = Ptrunc_valid[var]\n",
    "#         E3_valid_var[:,:,m] = E3_valid[var]\n",
    "        \n",
    "    \n",
    "    del X_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ptrunc['tas'].shape,Ptrunc['sic'].shape, E3['tas'].shape, nyears_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices for each variable:\n",
    "start = 0\n",
    "for k, var in enumerate(limvars): \n",
    "    print('working on '+var)\n",
    "    inds = var_dict[var]['var_ndof']\n",
    "    var_inds = np.arange(start,start+inds,1)\n",
    "    start = inds+start\n",
    "    \n",
    "    var_dict[var]['var_inds'] = var_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if load == 'full':\n",
    "#     ndof_all = limkb.count_ndof_all(limvars, E3, sic_separate=sic_separate)\n",
    "# elif load == 'truncated':\n",
    "#     if sic_separate is True:\n",
    "#         ndof_all = ((len(limvars)-1)*ntrunc)\n",
    "#     else:\n",
    "#         ndof_all = len(limvars)*ntrunc\n",
    "\n",
    "ndof_all = limkb.count_ndof_all(limvars, E3, sic_separate=sic_separate)\n",
    "\n",
    "[Ptrunc_all, E3_all, \n",
    "Ptrunc_sic,E_sic] = limkb.stack_variable_eofs(limvars, ndof_all, ntrunc, Ptrunc, E3, \n",
    "                                          var_dict, sic_separate=sic_separate)\n",
    "\n",
    "[P_train, Fvar, E] = limkb.step2_multivariate_compress(Ptrunc_all,nmodes, E3_all, Ptrunc_sic, \n",
    "                                                       sic_separate=sic_separate, Trunc_truth=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E3_all.shape, E.shape, E_sic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndof_all_valid = limkb.count_ndof_all(limvars, E3, sic_separate=sic_separate)\n",
    "\n",
    "[Ptrunc_all_valid, E3_all_valid,\n",
    " Ptrunc_sic_valid, E_sic_valid] = limkb.stack_variable_eofs(limvars, ndof_all_valid, ntrunc, Ptrunc_valid,\n",
    "                                                            E3, var_dict, sic_separate=sic_separate)\n",
    "\n",
    "[P_train_valid, Fvar_valid, \n",
    " E_valid] = limkb.step2_multivariate_compress(Ptrunc_all_valid,nmodes, E3_all_valid, Ptrunc_sic_valid,\n",
    "                                              sic_separate=sic_separate, Trunc_truth=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_valid.shape, E_sic_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ndof_all_valid = limkb.count_ndof_all(limvars, E3_valid, sic_separate=sic_separate)\n",
    "\n",
    "# [Ptrunc_all_valid2, E3_all_valid2,\n",
    "#  Ptrunc_sic_valid2, E_sic_valid2] = limkb.stack_variable_eofs(limvars, ndof_all_valid, ntrunc, Ptrunc, E3, \n",
    "#                                                             var_dict, sic_separate=sic_separate)\n",
    "\n",
    "# [P_train_valid2, Fvar_valid2, \n",
    "#  E_valid2] = limkb.step2_multivariate_compress(Ptrunc_all,nmodes, E3_all, Ptrunc_sic,\n",
    "#                                               sic_separate=sic_separate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmo = int(P_train.shape[1]/nyears_train)\n",
    "P_train_3d = np.reshape(P_train, (P_train.shape[0],nyears_train,nmo))\n",
    "\n",
    "if mo is 'all':\n",
    "    LIMd2, G2 = lim.LIM_train(tau,P_train)\n",
    "    print('Training LIM with tau = '+str(tau))\n",
    "else: \n",
    "    LIMd2, G2 = lim.LIM_train_flex(tau,P_train_3d[:,:,0], P_train_3d[:,:,1])\n",
    "    print('Training LIM with tau = '+str(tau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIM_save = {}\n",
    "# LIM_save['LIMd2'] = LIMd2\n",
    "# LIM_save['var_dict'] = var_dict\n",
    "# LIM_save['P_train_valid'] = P_train_valid\n",
    "# LIM_save['P_train'] = P_train\n",
    "# LIM_save['E'] = E\n",
    "# LIM_save['E_sic'] = E_sic\n",
    "# LIM_save['E_valid'] = E_valid\n",
    "# LIM_save['E_sic_valid'] = E_sic_valid\n",
    "# LIM_save['W_all'] = W_all\n",
    "# LIM_save['W_all_valid'] = W_all_valid\n",
    "# LIM_save['E_valid2'] = E_valid2\n",
    "# LIM_save['E_sic_valid2'] = E_sic_valid2\n",
    "\n",
    "# var_nms = [l+'_' for l in limvars]\n",
    "# savename = ('L_mo'+str(mo)+'_'+ ''.join(var_nms)+ 'ntrunc'+str(ntrunc)+'_nmodes'+str(nmodes)+\n",
    "#             '_nmodessic'+str(nmodes_sic)+'_ntrain'+str(nyears_train)+'_040521.pkl')\n",
    "# pickle.dump(LIM_save, open(savename, \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run building_L_single_month_040521.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-build L: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var_nms = [l+'_' for l in limvars]\n",
    "# savename = ('L_mo'+str(mo)+'_'+''.join(var_nms)+ 'ntrunc'+str(ntrunc)+\n",
    "#             '_nmodes'+str(nmodes)+'_nmodessic'+str(nmodes_sic)+'_ntrain'+str(nyears_train)+'_032321.pkl')\n",
    "# folder = 'L_tas_tos_sic_mpilm_ntrunc400_nmodes60_nmodessic_50_ntrain_2000_032321/'\n",
    "\n",
    "# LIM_save = pickle.load(open(folder+savename, \"rb\" ) )\n",
    "\n",
    "# LIMd2 = LIM_save['LIMd2']\n",
    "# var_dict = LIM_save['var_dict']\n",
    "# P_train_valid = LIM_save['P_train_valid'] \n",
    "# P_train = LIM_save['P_train']\n",
    "# E = LIM_save['E'] \n",
    "# E_sic = LIM_save['E_sic'] \n",
    "# E_valid = LIM_save['E_valid']\n",
    "# E_sic_valid = LIM_save['E_sic_valid']\n",
    "# W_all = LIM_save['W_all'] \n",
    "# W_all_valid = LIM_save['W_all_valid']\n",
    "# # X_valid_mn = LIM_save['X_valid_mn']\n",
    "# # X_train_mn = LIM_save['X_train_mn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Open truncated validation data: \n",
    "# var_nms = [l+'_' for l in limvars]\n",
    "# savename = ('P_train_valid_allmo_'+ ''.join(var_nms)+ 'ntrunc'+str(ntrunc)+\n",
    "#             '_nmodes'+str(nmodes)+'_nmodessic'+str(nmodes_sic)+'_031521.pkl')\n",
    "\n",
    "# P_save = pickle.load(open(savename, \"rb\" ) )\n",
    "\n",
    "# P_train_valid_allmo = P_save['Ptrain_valid_allmo']\n",
    "# Fvar_valid_allmo = P_save['Fvar_valid_allmo']\n",
    "# E_valid_allmo = P_save['E_valid_allmo'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Forecast: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIM forecasts for a range of monthly values (can specify a list of arbitrary values too)\n",
    "#lags = [0,1,2,3,4,5,6,7,8,9,10,11]\n",
    "lags = [0,1,2,3]\n",
    "#lags = [1]\n",
    "#lags = [0,3,6,12]\n",
    "ntims = len(lags)\n",
    "\n",
    "if mo == 'all':\n",
    "    LIM_fcast = limkb.LIM_forecast_Gt(LIMd2,P_train[:,0:nyears_train],lags)\n",
    "else: \n",
    "    P_train_2d = np.reshape(P_train, (P_train.shape[0],int(P_train.shape[1]/2),2))\n",
    "    \n",
    "    LIM_fcast = limkb.LIM_forecast_Gt(LIMd2,P_train_2d[:,:,0],lags)\n",
    "    \n",
    "#LIM_fcast = LIM_forecast_Gt(LIMd2,P_train_valid,lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(LIM_fcast['x_forecast'][0,0,:], label='leadtime: 0')\n",
    "plt.plot(LIM_fcast['x_forecast'][1,0,:], label='leadtime: 1')\n",
    "plt.plot(LIM_fcast['x_forecast'][2,0,:], label='leadtime: 2')\n",
    "plt.plot(LIM_fcast['x_forecast'][3,0,:], label='leadtime: 3')\n",
    "# plt.plot(LIM_fcast['x_forecast'][4,0,:])\n",
    "#plt.plot(LIM_fcast['x_forecast'][10,0,:])\n",
    "plt.xlim(0,20)\n",
    "plt.xticks(np.arange(0,400,5))\n",
    "plt.xlim(0,20)\n",
    "plt.ylim(-1.5,2.0)\n",
    "plt.xlabel('time')\n",
    "plt.xlabel('time')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIM_fcast['x_forecast'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0,600,1),LIM_fcast['x_forecast'][0,0,:], label='leadtime: 0')\n",
    "plt.plot(np.arange(0,599,1),LIM_fcast['x_forecast'][1,0,1:], label='leadtime: 1')\n",
    "plt.plot(np.arange(0,598,1),LIM_fcast['x_forecast'][2,0,2:], label='leadtime: 2')\n",
    "plt.plot(np.arange(0,597,1),LIM_fcast['x_forecast'][3,0,3:], label='leadtime: 3')\n",
    "# plt.plot(LIM_fcast['x_forecast'][4,0,:])\n",
    "#plt.plot(LIM_fcast['x_forecast'][10,0,:])\n",
    "plt.xlim(0,20)\n",
    "plt.xticks(np.arange(0,400,5))\n",
    "plt.xlim(0,20)\n",
    "plt.ylim(-1.5,2.0)\n",
    "plt.xlabel('time')\n",
    "plt.xlabel('time')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_all['tas'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_forecast_dcomp = np.zeros((len(lags),E.shape[0]+E_sic.shape[0],LIM_fcast['x_forecast'].shape[2]))\n",
    "\n",
    "for i,lag in enumerate(lags):\n",
    "    print('Lag '+ str(lag))\n",
    "    x_forecast_dcomp[i,:,:] = limkb.decompress_eof_separate_sic(LIM_fcast['x_forecast'][i,:,:],\n",
    "                                                                nmodes,nmodes_sic,E,\n",
    "                                                                E_sic,limvars,var_dict,\n",
    "                                                                W_all,Weights=True,\n",
    "                                                                sic_separate=sic_separate)\n",
    "    \n",
    "#units = areacell_dict[areawt_name[var]][areawt_name[var]]['units']                                                           W_all,Weights=True,sic_separate=sic_separate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_sic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIM_fcast['x_forecast'].shape, x_forecast_dcomp.shape, E.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_forecast_dcomp[0,var_dict['sic']['var_inds'],:]\n",
    "x_test1 = x_forecast_dcomp[1,var_dict['sic']['var_inds'],:]\n",
    "\n",
    "plt.plot(np.arange(0,600,1),x_test[0,:])\n",
    "plt.plot(np.arange(1,600,1),x_test1[0,1:])\n",
    "#plt.plot(np.arange(5,400,1),x_test1[0,5:])\n",
    "plt.xlim(0,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'sic'\n",
    "x_reshape = np.reshape(x_forecast_dcomp[1,var_dict[var]['var_inds'],:], (220,256,600))\n",
    "x_reshape2 = np.reshape(x_forecast_dcomp[0,var_dict[var]['var_inds'],:], (220,256,600))\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(12,5), subplot_kw=arc_proj )\n",
    "ax = ax.flatten()\n",
    "\n",
    "plotkb.sub_arctic_plot(ax[0],fig,x_reshape[:,:,20],var_dict[var]['lat'],var_dict[var]['lon'],\n",
    "                maxv=80,minv=-80,cmap='bwr')\n",
    "plotkb.sub_arctic_plot(ax[1],fig,x_reshape2[:,:,20],var_dict[var]['lat'],var_dict[var]['lon'],\n",
    "                maxv=80,minv=-80,cmap='bwr')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load = 'full'\n",
    "\n",
    "# v = {}\n",
    "# validvars = ['sic']\n",
    "# validation_stats = {}\n",
    "# valid_stats = {}\n",
    "\n",
    "# for k, var in enumerate(validvars): \n",
    "#     corr_tot = np.zeros((len(lags)))\n",
    "#     ce_tot = np.zeros((len(lags)))\n",
    "#     gm_var_ratio = np.zeros((len(lags)))\n",
    "    \n",
    "#     if load == 'full':\n",
    "#         print('Using full fields to validate.')\n",
    "# #        X_var, _ = limkb.load_data(var, v, fdic_ccsm4, remove_climo=True, detrend=True, verbose=True)\n",
    "        \n",
    "#     elif load == 'truncated': \n",
    "#         print('Using truncated fields to validate.')\n",
    "#         mod_fname = mod_filename\n",
    "#         [X_var, _, _, _, _] = load_truncated_data(var, mod_folder, mod_fname)\n",
    "\n",
    "#         tsamp = X_var.shape[1]\n",
    "#         nyears_train = int((tsamp*ntrain)/12)\n",
    "#         nyears_valid = int((tsamp*(1-ntrain))/12)\n",
    "\n",
    "#         X_t = np.reshape(X_var,(X_var.shape[0],int(tsamp/12),12))\n",
    "\n",
    "#         X_train_truth = X_t[:,0:nyears_train,mo+1]\n",
    "#         X_valid_truth = X_t[:,nyears_train:,mo+1]\n",
    "\n",
    "#     for i,lag in enumerate(lags):\n",
    "# #    for i,lag in enumerate([0]):\n",
    "#         print('Lag '+str(lag))\n",
    "        \n",
    "#         x_forecast = x_forecast_dcomp[lag,var_dict[var]['var_inds'],:] #+ X_valid_mn[:,np.newaxis]\n",
    "#         x_forecast_new_anom = x_forecast[:,lag:] - np.nanmean(x_forecast[:,lag:],axis=1)[:,np.newaxis]\n",
    "        \n",
    "#         tsamp = X_var.shape[1]\n",
    "#         ntime_train = x_forecast.shape[1]\n",
    "\n",
    "#         X_train_truth = X_t[:,0:nyears_train,mo+1]\n",
    "#         X_valid_truth = X_t[:,nyears_train:,mo+1]\n",
    "        \n",
    "#         if mo is 'all':\n",
    "#             print('Using all months...')\n",
    "#             X_t = X_var\n",
    "#             X_valid = X_t[:,0:ntime_train]\n",
    "            \n",
    "#             start_time = lag\n",
    "#             x_truth = X_t[:,start_time:ntime_train]\n",
    "#             x_truth_anom = x_truth - np.nanmean(x_truth,axis=1)[:,np.newaxis]\n",
    "            \n",
    "#         else: \n",
    "#             print('Using month '+str(mo)+'...')\n",
    "#             nyears_train = x_forecast.shape[1]\n",
    "#             #nyears_valid = int(X_all_mpi.shape[2]/12)\n",
    "#             nyears_valid = nyears_train\n",
    "            \n",
    "#             X_t = np.reshape(X_var,(X_var.shape[0],int(tsamp/12),12))\n",
    "            \n",
    "#             X_valid = X_t[:,0:nyears_train:,mo]\n",
    "         \n",
    "#             step = mo+lag\n",
    "#             print('Step = '+str(step))\n",
    "#             if step>11:\n",
    "#                 step = step-12\n",
    "#                 start_yr = 1\n",
    "#             else: \n",
    "#                 start_yr = 0\n",
    "            \n",
    "#             x_truth = X_t[:,start_yr:nyears_train-lag,step]\n",
    "#             x_truth_anom = x_truth - np.nanmean(x_truth,axis=1)[:,np.newaxis]\n",
    "                    \n",
    "#         units = areacell_dict[areawt_name[var]][areawt_name[var]]['units']\n",
    "#         tot_nh_sic_forecast = statskb.calc_tot_si(x_forecast_new_anom, areacell[areawt_name[var]], \n",
    "#                                                   units, var_dict[var]['lat'], lat_cutoff=0.0)\n",
    "#         tot_nh_sic_truth = statskb.calc_tot_si(x_truth_anom, areacell[areawt_name[var]], \n",
    "#                                                units, var_dict[var]['lat'],lat_cutoff=0.0)\n",
    "        \n",
    "#         plt.figure(figsize=(6,4))\n",
    "#         plt.plot(tot_nh_sic_truth*1e-6,label='truth')\n",
    "#         plt.plot(tot_nh_sic_forecast*1e-6,label='forecast')\n",
    "#         plt.xlim(0,10)\n",
    "#         plt.ylim(-1.5, 1.5)\n",
    "#         if mo is 'all':\n",
    "#             plt.title('All months')\n",
    "#         else:\n",
    "#             plt.title(month_names[step])\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "        \n",
    "#         if lag ==0: \n",
    "#             corr_tot[i] = np.corrcoef(tot_nh_sic_truth,tot_nh_sic_forecast)[0,1]\n",
    "#             ce_tot[i] = LMR_utils.coefficient_efficiency(tot_nh_sic_truth,tot_nh_sic_forecast)\n",
    "\n",
    "#             error_var = np.nanvar(x_truth_anom-x_forecast_new_anom,axis=1,ddof=1)\n",
    "#             truth_error_var = np.nanvar(x_truth_anom,axis=1,ddof=1)\n",
    "#         else: \n",
    "#             corr_tot[i] = np.corrcoef(tot_nh_sic_truth[:-lag],tot_nh_sic_forecast[lag:])[0,1]\n",
    "#             ce_tot[i] = LMR_utils.coefficient_efficiency(tot_nh_sic_truth[:-lag],tot_nh_sic_forecast[lag:])\n",
    "\n",
    "#             error_var = np.nanvar(x_truth_anom[:,:-lag]-x_forecast_new_anom[:,lag:],axis=1,ddof=1)\n",
    "#             truth_error_var = np.nanvar(x_truth_anom[:,:-lag],axis=1,ddof=1)\n",
    "            \n",
    "#         gm_error_var = statskb.global_mean(error_var,areacell[areawt_name[var]])\n",
    "#         gm_truth_var = statskb.global_mean(truth_error_var,areacell[areawt_name[var]])\n",
    "        \n",
    "#         gm_var_ratio[i]=gm_error_var/gm_truth_var\n",
    "        \n",
    "#     valid_stats['gm_var_ratio'] = gm_var_ratio\n",
    "#     valid_stats['corr_tot'] = corr_tot\n",
    "#     valid_stats['ce_tot'] = ce_tot\n",
    "        \n",
    "#     validation_stats[var] = valid_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = {}\n",
    "validvars = ['sic']\n",
    "validation_stats = {}\n",
    "valid_stats = {}\n",
    "\n",
    "for k, var in enumerate(validvars): \n",
    "    corr_tot = np.zeros((len(lags)))\n",
    "    ce_tot = np.zeros((len(lags)))\n",
    "    gm_var_ratio = np.zeros((len(lags)))\n",
    "\n",
    "#    X_var, _ = limkb.load_data(var, v, fdic_ccsm4, remove_climo=True, detrend=True, verbose=True)\n",
    "#    mo = 1\n",
    "    for i,lag in enumerate(lags):\n",
    "        print('Lag '+str(lag))\n",
    "        \n",
    "        x_forecast = x_forecast_dcomp[lag,var_dict[var]['var_inds'],:] #+ X_valid_mn[:,np.newaxis]\n",
    "        \n",
    "        tsamp = X_var.shape[1]\n",
    "        \n",
    "        if mo is 'all':\n",
    "            print('Using all months...')\n",
    "            X_t = X_var\n",
    "            x_truth = X_t[:,0:nyears_train]\n",
    "            x_truth_anom = x_truth - np.nanmean(x_truth,axis=1)[:,np.newaxis]\n",
    "            \n",
    "            x_forecast_new_anom = x_forecast[:,lag:] - np.nanmean(x_forecast[:,lag:],axis=1)[:,np.newaxis]\n",
    "            \n",
    "        else: \n",
    "            print('Using month '+str(mo)+'...')\n",
    "            nyears_train = int((tsamp*ntrain)/12)\n",
    "            nyears_valid = int((tsamp*(1-ntrain))/12)\n",
    "            \n",
    "            X_t = np.reshape(X_var,(X_var.shape[0],int(tsamp/12),12))\n",
    "            X_valid = X_t[:,nyears_train:,mo]\n",
    "                    \n",
    "            step = mo+lag\n",
    "            print('step = '+str(step))\n",
    "            if step>11:\n",
    "                step = step-12\n",
    "                start_yr = 0+1\n",
    "            else: \n",
    "                start_yr = 0\n",
    "\n",
    "            x_truth = X_t[:,start_yr:nyears_train,step]\n",
    "            x_truth_anom = x_truth - np.nanmean(x_truth,axis=1)[:,np.newaxis]\n",
    "            \n",
    "            x_forecast_new = np.reshape(x_forecast,(x_forecast.shape[0],100,12))[:,:,step]\n",
    "            x_forecast_new_anom = x_forecast_new - np.nanmean(x_forecast_new,axis=1)[:,np.newaxis]\n",
    "        \n",
    "#         if var is 'sic':\n",
    "#             x_forecast_noneg = np.where(x_forecast<-100.0, -100.0, x_forecast)\n",
    "#             x_forecast_new = np.where(x_forecast_noneg>100.0, 100.0, x_forecast_noneg)\n",
    "#         else: \n",
    "#             x_forecast_new = x_forecast\n",
    "    \n",
    "        units = areacell_dict[areawt_name[var]][areawt_name[var]]['units']\n",
    "        tot_nh_sic_forecast = statskb.calc_tot_si(x_forecast_new_anom, areacell[areawt_name[var]], \n",
    "                                          units, var_dict[var]['lat'], lat_cutoff=0.0)\n",
    "        tot_nh_sic_truth = statskb.calc_tot_si(x_truth_anom, areacell[areawt_name[var]], \n",
    "                                       units, var_dict[var]['lat'],lat_cutoff=0.0)\n",
    "        \n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.plot(tot_nh_sic_truth*1e-6,label='truth')\n",
    "        plt.plot(tot_nh_sic_forecast*1e-6,label='forecast')\n",
    "        plt.xlim(0,10)\n",
    "        plt.ylim(-1.5,1.5)\n",
    "        plt.legend()\n",
    "        plt.title(month_names[step])\n",
    "        plt.show()\n",
    "        \n",
    "        if lag ==0: \n",
    "            corr_tot[i] = np.corrcoef(tot_nh_sic_truth,tot_nh_sic_forecast)[0,1]\n",
    "            ce_tot[i] = LMR_utils.coefficient_efficiency(tot_nh_sic_truth,tot_nh_sic_forecast)\n",
    "\n",
    "            error_var = np.nanvar(x_truth_anom-x_forecast_new_anom,axis=1,ddof=1)\n",
    "            truth_error_var = np.nanvar(x_truth_anom,axis=1,ddof=1)\n",
    "        else: \n",
    "            corr_tot[i] = np.corrcoef(tot_nh_sic_truth[:-lag],tot_nh_sic_forecast)[0,1]\n",
    "            ce_tot[i] = LMR_utils.coefficient_efficiency(tot_nh_sic_truth[:-lag],tot_nh_sic_forecast)\n",
    "\n",
    "            error_var = np.nanvar(x_truth_anom[:,:-lag]-x_forecast_new_anom,axis=1,ddof=1)\n",
    "            truth_error_var = np.nanvar(x_truth_anom[:,:-lag],axis=1,ddof=1)\n",
    "            \n",
    "        gm_error_var = statskb.global_mean(error_var,areacell[areawt_name[var]])\n",
    "        gm_truth_var = statskb.global_mean(truth_error_var,areacell[areawt_name[var]])\n",
    "        \n",
    "        gm_var_ratio[i]=gm_error_var/gm_truth_var\n",
    "        \n",
    "        print(np.var(x_forecast_new_anom[1060,:]))\n",
    "    valid_stats['gm_var_ratio'] = gm_var_ratio\n",
    "    valid_stats['corr_tot'] = corr_tot\n",
    "    valid_stats['ce_tot'] = ce_tot\n",
    "        \n",
    "    validation_stats[var] = valid_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ^^^ Variance is not decreasing with lag at this and other gridcells? ^^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_yr, nyears_train, step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_truth_anom[1061,:])\n",
    "plt.plot(x_forecast_new_anom[1061,:])\n",
    "plt.xlim(0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_var = x_truth\n",
    "var_nans_mask = np.where(np.isnan(x_var),np.nan,1)\n",
    "var_dt = spy.signal.detrend(np.where(np.isnan(x_var),0,x_var),axis=0)\n",
    "x_var_dt = var_dt*var_nans_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testmn = np.nanmean(x_var_dt, axis=1)\n",
    "testmn[1061]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_var_dt[1061,:])\n",
    "plt.plot(x_var[1061,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_sic_2d = np.reshape(error_var, (var_dict['sic']['lat'].shape))\n",
    "plt.pcolormesh(error_sic_2d)\n",
    "plt.colorbar()\n",
    "plt.title('Error Variance: SIC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_sic_2d = np.reshape(truth_error_var, (var_dict['sic']['lat'].shape))\n",
    "plt.pcolormesh(error_sic_2d)\n",
    "plt.colorbar()\n",
    "plt.title('Truth Variance: SIC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(valid_stats['gm_var_ratio'])\n",
    "plt.axhline(1,linestyle='--', color='k')\n",
    "plt.xticks([0,1,2,3])\n",
    "plt.ylim(0,1.1)\n",
    "plt.xlabel('lead time (months)')\n",
    "plt.title('All months')\n",
    "plt.ylabel('GM ratio: error variance to true variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(valid_stats['corr_tot']**2, label='Correlation')\n",
    "plt.plot(valid_stats['ce_tot'],label='CE')\n",
    "#plt.axhline(0,linestyle='--', color='k')\n",
    "plt.ylim(0,1)\n",
    "plt.xlim(0,4)\n",
    "plt.xticks([0,1,2,3])\n",
    "plt.grid(axis='both')\n",
    "plt.xlabel('lead time (months)')\n",
    "plt.title(month_names[mo])\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tot_nh_sic_forecast*1e-6, label='forecast')\n",
    "plt.plot((tot_nh_sic_truth-np.mean(tot_nh_sic_truth,axis=0))*1e-6, label='truth')\n",
    "#plt.plot(tot_nh_sic_forecast[3:]*1e-6, label='forecast')\n",
    "plt.xlim(0,50)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var((tot_nh_sic_truth-np.mean(tot_nh_sic_truth,axis=0))*1e-6), np.var(tot_nh_sic_forecast[1:]*1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.corrcoef(tot_nh_sic_truth,tot_nh_sic_forecast)[0,1]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validvars = ['sic']\n",
    "# v = {}\n",
    "\n",
    "# for k, var in enumerate(validvars): \n",
    "#     X_var, _ = load_data(var, v, fdic_ccsm4, remove_climo=True, detrend=True, verbose=True)\n",
    "    \n",
    "#     mo = 1\n",
    "#     tsamp = X_var.shape[1]\n",
    "#     nyears_train = int((tsamp*ntrain)/12)\n",
    "#     nyears_valid = int((tsamp*(1-ntrain))/12)\n",
    "\n",
    "#     X_t = np.reshape(X_var,(X_var.shape[0],int(tsamp/12),12))\n",
    "#     x_truth = X_t[:,nyears_train:,mo+1]\n",
    "    \n",
    "#     x_forecast = x_forecast_dcomp[var_dict[var]['var_inds'],:]\n",
    "    \n",
    "#     units = areacell_dict[areawt_name[var]][areawt_name[var]]['units']\n",
    "#     tot_nh_sic_forecast = calc_tot_si(x_forecast, areacell[areawt_name[var]], \n",
    "#                                       units, var_dict[var]['lat'], cutoff=0.0)\n",
    "#     tot_nh_sic_truth = calc_tot_si(x_truth, areacell[areawt_name[var]], \n",
    "#                                    units, var_dict[var]['lat'],cutoff=0.0)\n",
    "    \n",
    "#     corr_tot_sic = np.corrcoef(tot_nh_sic_truth,tot_nh_sic_forecast)[0,1]\n",
    "#     ce_tot_sic = LMR_utils.coefficient_efficiency(tot_nh_sic_truth,tot_nh_sic_forecast)\n",
    "    \n",
    "#     error_var = np.nanvar(x_forecast - x_truth,axis=1,ddof=1)\n",
    "#     gm_error_var = global_mean(error_var,areacell[areawt_name[var]])\n",
    "    \n",
    "#     truth_error_var = np.nanvar(x_truth,axis=1,ddof=1)\n",
    "#     gm_truth_var = global_mean(truth_error_var,areacell[areawt_name[var]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_stats['sic']['gm_var_ratio'].shape, validation_stats['sic']['corr_tot'].shape, validation_stats['sic']['ce_tot'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(validation_stats['sic']['gm_var_ratio'])\n",
    "plt.axhline(1,linestyle='--', color='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(validation_stats['sic']['corr_tot']**2, label='Correlation')\n",
    "plt.plot(validation_stats['sic']['ce_tot'],label='CE')\n",
    "#plt.axhline(0,linestyle='--', color='k')\n",
    "plt.ylim(0,1)\n",
    "plt.xlim(0,4)\n",
    "plt.grid(axis='both')\n",
    "plt.xlabel('lead time (months)')\n",
    "plt.title(month_names[mo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_stats['sic']['corr_tot'], validation_stats['sic']['ce_tot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_stats['sic'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_stats['sic']['corr_tot'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMd['G']**t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arctic_mean(var, areacell, cutoff=0.0): \n",
    "    tot_nh_var = var*areacell\n",
    "    if len(lat.shape)<=1:\n",
    "        lat_inds = np.where(var_dict[var]['lat']>cutoff)\n",
    "        tot_nh_var = np.nansum(np.nansum(tot_nh_var[:,lat_inds,:],axis=1),axis=1)\n",
    "    \n",
    "        wt_sum = np.nansum(np.nansum(cellarea[lat_inds,:],axis=0),axis=0)\n",
    "    else:\n",
    "        lat_inds = np.where(var_dict[var]['lat']>cutoff)\n",
    "        tot_nh_var = np.nansum(np.nansum(tot_nh_var[:,lat_inds],axis=1),axis=1)\n",
    "    \n",
    "        wt_sum = np.nansum(np.nansum(cellarea[lat_inds],axis=0),axis=0)\n",
    "    \n",
    "    var_mn = tot_nh_var/wt_sum\n",
    "    \n",
    "    return var_mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_mean(var, areacell): \n",
    "    \"\"\"Assumes var is dimensions (nlat*nlon,time)\n",
    "    \"\"\"\n",
    "    \n",
    "    tot_nh_var = var*areacell\n",
    "    \n",
    "    tot_var = np.nansum(tot_nh_var,axis=0)\n",
    "    wt_sum = np.nansum(areacell,axis=0)\n",
    "    \n",
    "    var_mn = tot_var/wt_sum\n",
    "    \n",
    "    return var_mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_tot_si(var, areacell, units, lat, cutoff=0.0): \n",
    "    if units == 'm2':\n",
    "        cellarea = (areacell*1e-6)[:,np.newaxis]\n",
    "    else: \n",
    "        cellarea = areacell[:,np.newaxis]\n",
    "        \n",
    "    if var.max()>2:\n",
    "        Var = var/100.0\n",
    "    else: \n",
    "        Var = var\n",
    "        \n",
    "    nh_var = Var*cellarea\n",
    "    \n",
    "    if len(lat.shape)<=1:\n",
    "        lat_inds = np.where(lat>cutoff)\n",
    "        tot_nh_var = np.nansum(nh_var[lat_inds,:].squeeze(),axis=0)\n",
    "    else:\n",
    "        lat_1d = np.reshape(lat,(var.shape[0]))\n",
    "        lat_inds = np.where(lat_1d>cutoff)\n",
    "        tot_nh_var = np.nansum(nh_var[lat_inds,:].squeeze(),axis=0)\n",
    "    \n",
    "    return tot_nh_var\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gm_polar_variance(valid_var,valid_var_mon,fields,lat,lon):\n",
    "    valid_variance = {}\n",
    "    gm_mon = np.zeros((12))\n",
    "    polar_mon = np.zeros((2,12))\n",
    "    nlat = lat.shape[0]\n",
    "    nlon = lon.shape[0]\n",
    "\n",
    "    for v in fields.keys():\n",
    "        print(v)\n",
    "        var = np.reshape(valid_var[fields[v]],[nlat,nlon])\n",
    "        var_mon = np.reshape(valid_var_mon[fields[v],:].T,[12,nlat,nlon])\n",
    "\n",
    "        gm,_,_ = LMR_utils.global_hemispheric_means(var,lat)\n",
    "        polar = polar_regional_means(var,lat,lon)\n",
    "        polar_mon = polar_regional_means(var_mon,lat,lon)\n",
    "        for m in range(12):\n",
    "            gm_mon[m],_,_ = LMR_utils.global_hemispheric_means(var_mon[m,:,:],lat)\n",
    "\n",
    "        valid_variance[v+'_gm'] = gm\n",
    "        valid_variance[v+'_gm_mon'] = gm_mon\n",
    "        valid_variance[v+'_polarm'] = polar\n",
    "        valid_variance[v+'_polarm_mon'] = polar_mon\n",
    "        \n",
    "    return valid_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LIM_forecast(LIMd,x,lags):\n",
    "    \"\"\"\n",
    "    # There is a bug with this forecast function function: It uses the eigenvectors and \n",
    "    #        values to calculate Gt, but it's giving the same value for all lags in the forecast\n",
    "    \n",
    "    deterministic forecasting experiments for states in x and time lags in lags.\n",
    "\n",
    "    Inputs:\n",
    "    * LIMd: a dictionary with LIM attributes\n",
    "    * x: a state-time matrix for initial conditions and verification ~(ndof,ntims)\n",
    "    * lags: list of time lags for deterministic forecasts\n",
    "    * E: the linear map from the coordinates of the LIM to physical (lat,lon) coordinates ~(nx*ny,ndof)\n",
    "    \n",
    "    Outputs (in a dictionary):\n",
    "    *'error' - error variance as a function of space and forecast lead time (ndof,ntims)\n",
    "    *'x_forecast' - the forecast states (nlags,ndof,ntims)\n",
    "    *'x_truth_phys_space' - true state in physical space (nlat*nlon,*ntims)\n",
    "    *'x_forecast_phys_space' - forecast state in physical space (nlat*nlon,*ntims)\n",
    "    \"\"\"\n",
    "    \n",
    "    ndof = x.shape[0]\n",
    "    ntims = x.shape[1]\n",
    "    nlags = len(lags)\n",
    "    nx = E.shape[0]\n",
    "    LIMfd = {}\n",
    "    \n",
    "    error = np.zeros([nx,nlags])\n",
    "    x_predict_save = np.zeros([nlags,ndof,ntims])\n",
    "    \n",
    "    for k,t in enumerate(lags):\n",
    "        print('t=',t)\n",
    "        # make the propagator for this lead time\n",
    " #       Gt = np.matmul(np.matmul(LIMd['vec'],np.diag(np.exp(LIMd['lam']*t))),LIMd['veci'])\n",
    "        Gt = np.matmul(np.matmul(LIMd['vec'],np.diag(np.exp(LIMd['lam']*t))),LIMd['veci'])\n",
    "        \n",
    "        # forecast\n",
    "        if t == 0:\n",
    "            # need to handle this time separately, or the matrix dimension is off\n",
    "            x_predict = np.matmul(Gt,x)\n",
    "            x_predict_save[k,:,:] = x_predict\n",
    "        else:\n",
    "            x_predict = np.matmul(Gt,x[:,:-t])\n",
    "            x_predict_save[k,:,t:] = x_predict\n",
    "\n",
    "        # physical-space fields for forecast and truth for this forecast lead time ~(ndof,ntims)\n",
    "#        X_predict = np.real(np.matmul(E,x_predict))\n",
    "        #X_truth = np.real(np.matmul(E,x[:,t:]))\n",
    "#         X_truth = truth[:,t:]\n",
    "        \n",
    "#         # error variance as a function of space and forecast lead time ~(ndof,ntims)\n",
    "#         error[:,k] = np.var(X_predict - X_truth,axis=1,ddof=1)\n",
    "    \n",
    "        # return the LIM forecast error dictionary\n",
    "#         LIMfd['error'] = error\n",
    "        Ld = {}\n",
    "        Ld['Gt'] = Gt\n",
    "        LIMfd[t] = Ld\n",
    "    \n",
    "    LIMfd['x_forecast'] = np.squeeze(x_predict_save)    \n",
    "        \n",
    "    return LIMfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LIM_forecast_Gt(LIMd,x,lags):\n",
    "    \"\"\"\n",
    "    deterministic forecasting experiments for states in x and time lags in lags.\n",
    "\n",
    "    Inputs:\n",
    "    * LIMd: a dictionary with LIM attributes\n",
    "    * x: a state-time matrix for initial conditions and verification ~(ndof,ntims)\n",
    "    * lags: list of time lags for deterministic forecasts\n",
    "    * E: the linear map from the coordinates of the LIM to physical (lat,lon) coordinates ~(nx*ny,ndof)\n",
    "    \n",
    "    Outputs (in a dictionary):\n",
    "    *'error' - error variance as a function of space and forecast lead time (ndof,ntims)\n",
    "    *'x_forecast' - the forecast states (nlags,ndof,ntims)\n",
    "    *'x_truth_phys_space' - true state in physical space (nlat*nlon,*ntims)\n",
    "    *'x_forecast_phys_space' - forecast state in physical space (nlat*nlon,*ntims)\n",
    "    \"\"\"\n",
    "    \n",
    "    ndof = x.shape[0]\n",
    "    ntims = x.shape[1]\n",
    "    nlags = len(lags)\n",
    "    nx = E.shape[0]\n",
    "    LIMfd = {}\n",
    "    \n",
    "    error = np.zeros([nx,nlags])\n",
    "    x_predict_save = np.zeros([nlags,ndof,ntims])\n",
    "    \n",
    "    for k,t in enumerate(lags):\n",
    "        print('t=',t)\n",
    "        # make the propagator for this lead time\n",
    " #       Gt = np.matmul(np.matmul(LIMd['vec'],np.diag(np.exp(LIMd['lam']*t))),LIMd['veci'])\n",
    "        Gt = np.linalg.matrix_power(LIMd['G'],t)\n",
    "        # forecast\n",
    "        if t == 0:\n",
    "            # need to handle this time separately, or the matrix dimension is off\n",
    "            x_predict = np.matmul(Gt,x)\n",
    "            x_predict_save[k,:,:] = x_predict\n",
    "        else:\n",
    "            x_predict = np.matmul(Gt,x[:,:-t])\n",
    "            x_predict_save[k,:,t:] = x_predict\n",
    "\n",
    "        Ld = {}\n",
    "        Ld['Gt'] = Gt\n",
    "        LIMfd[t] = Ld\n",
    "    \n",
    "    LIMfd['x_forecast'] = np.squeeze(x_predict_save)    \n",
    "        \n",
    "    return LIMfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yrs_mo = np.reshape(var_dict[var]['time'],(int(tsamp/12),12))\n",
    "\n",
    "yrs_train = yrs_mo[0:nyears_train,mo]\n",
    "yrs_valid = yrs_mo[nyears_train:,mo+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EOF decomposition: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[Ptrunc, E3, Ptrunc_sic, E_sic,\n",
    " W_train, sf_train,\n",
    " tot_var, tot_var_eig] = limkb.step1_compress_individual_vars(X_train, limvars, ntrunc, \n",
    "                                                              nmodes_sic, var_dict,X_all.shape[0],\n",
    "                                                              var_dict['sic']['var_ndof'], \n",
    "                                                              wt=True,sic_separate=sic_separate)\n",
    "\n",
    "\n",
    "[P_train, Fvar, E] = limkb.step2_multivariate_compress(Ptrunc,nmodes, E3, Ptrunc_sic, \n",
    "                                                       sic_separate=sic_separate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[Ptrunc2, E3_2, Ptrunc_sic2, \n",
    " E_sic2, W_train2, sf_train2] = limkb.compress_individual_vars2(X_train2, limvars, ntrunc, nmodes_sic, var_dict, \n",
    "                                                                X_all.shape[0], var_dict['sic']['var_ndof'], \n",
    "                                                                sic_separate=sic_separate)\n",
    "    \n",
    "[P_train2, Fvar2, E2] = limkb.step2_multivariate_compress(Ptrunc2,nmodes, E3_2, \n",
    "                                                          Ptrunc_sic2, sic_separate=sic_separate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[Ptrunc_valid, E3_valid, Ptrunc_sic_valid, \n",
    " E_sic_valid, W_valid, sf_valid] = limkb.compress_individual_vars2(X_valid, limvars, ntrunc, nmodes_sic, \n",
    "                                                                   var_dict,X_all.shape[0], \n",
    "                                                                   var_dict['sic']['var_ndof'], \n",
    "                                                                   sic_separate=sic_separate)\n",
    "\n",
    "[P_valid, Fvar_valid, E_valid] = limkb.step2_multivariate_compress(Ptrunc_vlaid,nmodes, E3_valid, \n",
    "                                                                   Ptrunc_sic_valid, sic_separate=sic_separate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data to train and test the LIM. \n",
    "# separate data into training the validation sets \n",
    "tsamp = X_all.shape[1]\n",
    "nyears = tsamp/12\n",
    "ntrain_yrs = int(np.floor((tsamp/12)*ntrain)*12)\n",
    "train_inds = np.arange(0,ntrain_yrs)\n",
    "valid_inds = np.arange(ntrain_yrs,tsamp,1)\n",
    "\n",
    "x_train = P_train\n",
    "x_train2 = P_train2\n",
    "x_valid = P_valid\n",
    "\n",
    "print('training data shape: ',x_train.shape)\n",
    "print('validation data shape: ',x_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_dcomp = decompress_eof_separate_sic2(P_train,nmodes,nmodes_sic,E,E_sic,\n",
    "#                                              limvars,var_dict,W_train,Weights=True,\n",
    "#                                              sic_separate=sic_separate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # climo standardized variance for error normalization \n",
    "# #  in training and validation (1=no skill)\n",
    "# # EOF space only\n",
    "# if sic_separate is True: \n",
    "#     E_x_train = lim.decompress_eof_separate_sic(x_train,nmodes,nmodes_sic,E,E_sic)\n",
    "#     E_x_valid = lim.decompress_eof_separate_sic(x_valid,nmodes,nmodes_sic,E_valid,E_sic_valid)\n",
    "\n",
    "# else: \n",
    "#     E_x_train = np.matmul(E,x_train)\n",
    "#     E_x_valid = np.matmul(E_valid,x_valid)\n",
    "\n",
    "# E_x_train[np.isclose(E_x_train,0,atol=10e-10)] = np.nan\n",
    "# E_x_valid[np.isclose(E_x_valid,0,atol=10e-10)] = np.nan\n",
    "    \n",
    "# nyrs_train = int(E_x_train.shape[1]/12)\n",
    "# nyrs_valid = int(E_x_valid.shape[1]/12)\n",
    "# nlalo = E_x_train.shape[0]\n",
    "\n",
    "# # train_var_mon = np.nanvar(np.reshape(E_x_train,(nlalo,nyrs_train,12)),\n",
    "# #                        axis=1,ddof=1)\n",
    "# # valid_var_mon = np.nanvar(np.reshape(E_x_valid,(nlalo,nyrs_valid,12)),\n",
    "# #                        axis=1,ddof=1)\n",
    "# train_var = np.nanvar(E_x_train,axis=1,ddof=1)\n",
    "# valid_var = np.nanvar(E_x_valid,axis=1,ddof=1)\n",
    "\n",
    "# # FULL FIELD versions of the variance \n",
    "# train_var_full = np.nanvar(X_train,axis=1,ddof=1)\n",
    "# valid_var_full = np.nanvar(X_valid,axis=1,ddof=1)\n",
    "\n",
    "# # valid_var_mon_full = np.var(np.reshape(X_mpi,(nvars*ndof,nyrs_valid,12)),\n",
    "# #                             axis=1,ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the LIM\n",
    "LIMd, G = lim.LIM_train(tau,x_train)\n",
    "print('Training LIM with tau = '+str(tau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMd2, G2 = lim.LIM_train_flex(tau,x_train, x_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIM forecasts for a range of monthly values (can specify a list of arbitrary values too)\n",
    "lags = [1]\n",
    "#lags = [0,3,6,12]\n",
    "ntims = len(lags)\n",
    "\n",
    "# training data\n",
    "#truth = np.real(np.matmul(E,x_train))\n",
    "#truth = X[:,train_inds]\n",
    "#LIMfd_train = LIM_utils.LIM_forecast(LIMd,x_train,lags,E,truth)\n",
    "\n",
    "# validation data\n",
    "#truth = np.real(np.matmul(E,x_valid))\n",
    "LIMfd_valid = lim.LIM_forecast_test(G,x_valid,lags,E_valid,truth,yrs_valid,len(limvars),ndof,\n",
    "                                    nmodes=nmodes,nmodes_sic=nmodes_sic,E_sic=E_sic_valid,\n",
    "                                    sic_separate=sic_separate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_state = {}\n",
    "nlat = lat_2d[:,0].shape[0]\n",
    "nlon = lon_2d[0,:].shape[0]\n",
    "\n",
    "for v,var in enumerate(limvars):\n",
    "    print('working on '+str(var))\n",
    "    if sic_separate is True: \n",
    "        Truth = lim.decompress_eof(truth,E,E_sic=E_sic,\n",
    "                                   nmodes=nmodes,nmodes_sic=nmodes_sic,\n",
    "                                   sic_separate=sic_separate)\n",
    "    else: \n",
    "        Truth = np.matmul(E,truth)\n",
    "#            print(Truth.shape)\n",
    "    truth_3d = np.reshape(Truth[fields[var],:].T,[Truth.shape[1],nlat,nlon])\n",
    "    truth_gm,_,_ = LMR_utils.global_hemispheric_means(truth_3d[1:,:,:],lat_2d[:,0])\n",
    "    truth_pm = lim.polar_regional_means(truth_3d[1:,:,:],lat_2d[:,0],lon_2d[0,:])\n",
    "\n",
    "    truth_state[var+'_gm_mo_'+str(mo)] = truth_gm\n",
    "    truth_state[var+'_pm_mo_'+str(mo)] = truth_pm\n",
    "    truth_state[var+'_full_mo_'+str(mo)] = truth_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMfd_valid['x_forecast'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = {}\n",
    "\n",
    "for v,var in enumerate(limvars):\n",
    "    print('working on '+str(var))\n",
    "    if sic_separate is True: \n",
    "        fcast = lim.decompress_eof(LIMfd_valid['x_forecast'][0,:,:],E,E_sic=E_sic,\n",
    "                                      nmodes=nmodes,nmodes_sic=nmodes_sic,\n",
    "                                      sic_separate=sic_separate)\n",
    "    else: \n",
    "        fcast = np.matmul(E,LIMfd_valid['x_forecast'][0,:,:])\n",
    "#            print(Truth.shape)\n",
    "    forecast_3d = np.reshape(fcast[fields[var],:].T,[fcast.shape[1],nlat,nlon])\n",
    "    forecast_gm,_,_ = LMR_utils.global_hemispheric_means(forecast_3d[1:,:,:],lat_2d[:,0])\n",
    "    forecast_pm = lim.polar_regional_means(forecast_3d[1:,:,:],lat_2d[:,0],lon_2d[0,:])\n",
    "\n",
    "    forecast[var+'_gm_mo_'+str(mo)] = forecast_gm\n",
    "    forecast[var+'_pm_mo_'+str(mo)] = forecast_pm\n",
    "    forecast[var+'_full_mo_'+str(mo)] = forecast_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce = {}\n",
    "corr = {}\n",
    "\n",
    "for n,var in enumerate(limvars):\n",
    "    print('working on '+str(var))\n",
    "    ce_pm_lag = np.zeros((2))\n",
    "    corr_pm_lag = np.zeros((2))\n",
    "\n",
    "    ce_gm = LMR_utils.coefficient_efficiency(truth_state[var+'_gm_mo_'+str(mo)],\n",
    "                                                    forecast[var+'_gm_mo_'+str(mo)])\n",
    "    corr_gm = np.corrcoef(truth_state[var+'_gm_mo_'+str(mo)],\n",
    "                                 forecast[var+'_gm_mo_'+str(mo)])[0,1]\n",
    "\n",
    "    for n in range(2):\n",
    "        ce_pm_lag[n] = LMR_utils.coefficient_efficiency(truth_state[var+'_pm_mo_'+str(mo)][n,:],\n",
    "                                                          forecast[var+'_pm_mo_'+str(mo)][n,:])\n",
    "        corr_pm_lag[n] = np.corrcoef(truth_state[var+'_pm_mo_'+str(mo)][n,:],\n",
    "                                       forecast[var+'_pm_mo_'+str(mo)][n,:])[0,1]\n",
    "\n",
    "    ce[var+'_gm_mo_'+str(mo)] = ce_gm\n",
    "    ce[var+'_pm_mo_'+str(mo)] = ce_pm_lag\n",
    "    corr[var+'_gm_mo_'+str(mo)] = corr_gm\n",
    "    corr[var+'_pm_mo_'+str(mo)] = corr_pm_lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(forecast['tas_gm_mo_'+str(mo)])\n",
    "plt.plot(truth_state['tas_gm_mo_'+str(mo)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in limvars: \n",
    "    print(var)\n",
    "    print('GM CE = '+str(ce[var+'_gm_mo_'+str(mo)])+' GM Corr = '+ str(corr[var+'_gm_mo_'+str(mo)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_ar1, corr_ar1= lim.calc_ce_corr_ar_lags(X_valid,X_train,lags,limvars,\n",
    "                                           lat_2d[:,0],lon_2d[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_ar1_valid, corr_ar1_valid = lim.calc_ce_corr_ar_lags(X_valid,X_valid,lags,\n",
    "                                                        limvars,lat_2d[:,0], lon_2d[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = 3\n",
    "var = 'tos'\n",
    "Xt = X_valid\n",
    "Xv = X_valid\n",
    "lat = lat_2d[:,0]\n",
    "lon = lon_2d[0,:]\n",
    "l = np.arange(1,2,1)\n",
    "\n",
    "ar1_gm = np.zeros((len(l),Xv.shape[2]))\n",
    "ar1_pm = np.zeros((len(l),2,Xv.shape[2]))\n",
    "ce_pm = np.zeros((len(l),2))\n",
    "corr_pm = np.zeros((len(l),2))\n",
    "\n",
    "for i,lag in enumerate(l):\n",
    "    ar1_forecast, ar1_factor = lim.red_noise_forecast_ar1(Xt[v,:].T,Xv[v,:].T,lead=lag)\n",
    "    ar1_forecast_3d = np.reshape(ar1_forecast,(ar1_forecast.shape[0],nlat,nlon))\n",
    "    X_valid_3d = np.reshape(Xv[v,:].T,(Xv.shape[2],nlat,nlon))\n",
    "    ar1_gm[i,lag:],_,_ = LMR_utils.global_hemispheric_means(ar1_forecast_3d,lat)\n",
    "    ar1_pm[i,:,lag:] = lim.polar_regional_means(ar1_forecast_3d,lat,lon)\n",
    "\n",
    "    true_gm,_,_ = LMR_utils.global_hemispheric_means(X_valid_3d,lat)\n",
    "    true_pm = lim.polar_regional_means(X_valid_3d,lat,lon)\n",
    "\n",
    "    ce_gm = LMR_utils.coefficient_efficiency(true_gm[lag:],ar1_gm[i,lag:])\n",
    "    corr_gm = np.corrcoef(true_gm[lag:],ar1_gm[i,lag:])[0,1]\n",
    "    for n in range(2):\n",
    "        ce_pm[i,n] = LMR_utils.coefficient_efficiency(true_pm[n,lag:],ar1_pm[i,n,lag:])\n",
    "        corr_pm[i,n] = np.corrcoef(true_pm[n,lag:],ar1_pm[i,n,lag:])[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmin(ar1_factor), Xv[v,:].T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar1_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_gm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(true_gm[1:],ar1_gm[0,1:])[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(true_gm[1:], label='True')\n",
    "plt.plot(ar1_gm[0,2:], label='AR1')\n",
    "plt.xlim(0,100)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check in eof space: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_eof = np.matmul(E_sic,x_valid_sic)\n",
    "truth_eof_gm,_,_ = LMR_utils.global_hemispheric_means(np.reshape(truth_eof.T,(7212,45,72)), lat_2d[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_state['sic_gm'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(truth_state['sic_gm'][0,:],label='truth')\n",
    "plt.plot(forecast['sic_gm'][0,:],label='forecast')\n",
    "plt.plot(truth_eof_gm,label='truth eof')\n",
    "plt.legend()\n",
    "plt.title('GM SIC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0\n",
    "ce_test = LMR_utils.coefficient_efficiency(truth_eof_gm,forecast[var+'_gm'][k,k:])\n",
    "corr_test = np.corrcoef(truth_eof_gm,forecast[var+'_gm'][k,k:])[0,1]\n",
    "corr_test, ce_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_ar1_valid['tos_gm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Detrending: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'sic'\n",
    "wt=True\n",
    "var_dict = {}\n",
    "\n",
    "X_var_dt, var_dict = limkb.load_data(var, var_dict, fdic_ccsm4, remove_climo=False, \n",
    "                                     detrend=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timei = np.arange(0,var_dict['tas']['time'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_var[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats \n",
    "\n",
    "gradient, intercept, r_value, p_value, std_err = stats.linregress(timei,X_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient.shape, intercept.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = np.var(X_var[0,:])\n",
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_anom = X_var - X_var.mean(axis=1)[:,np.newaxis]\n",
    "X_anom = timei - timei.mean()\n",
    "X_anom_mat = (X_anom * np.ones_like(Y_anom))\n",
    "\n",
    "cov = np.dot(X_anom_mat.T,Y_anom)/(len(Y_anom)-1)\n",
    "var = np.var(timei)\n",
    "\n",
    "# a = cov/var\n",
    "# b = X_var[0,:].mean() - (a)*timei.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_linear_fit(X,Y):\n",
    "    X_anom = X - X.mean()\n",
    "    Y_anom = Y - Y.mean()\n",
    "    \n",
    "    cov = np.dot(X_anom, Y_anom)/(len(Y_anom)-1)\n",
    "    var = np.var(X)\n",
    "    \n",
    "    slope = cov/var\n",
    "    intercept = Y.mean() - slope*X.mean()\n",
    "    \n",
    "    return slope, intercept "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_detrend(X,Y,remove_mn=False):\n",
    "    slope, intercept = calc_linear_fit(X,Y)\n",
    "    \n",
    "    lin_fit = X*slope + intercept \n",
    "    \n",
    "    Y_dt = Y - lin_fit\n",
    "    \n",
    "    if remove_mn is False: \n",
    "        Y_dt_out = Y_dt + intercept\n",
    "    else: \n",
    "        Y_dt_out = Y_dt \n",
    "        \n",
    "    return Y_dt_out, slope, intercept "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_linear_detrend(X,Y,axis=1,remove_mn=False):\n",
    "    \"\"\"\n",
    "    axis = dimension which to detrend over. \n",
    "    \"\"\"\n",
    "    if len(Y.shape)>2:\n",
    "        raise ValueError('Too many dimensions in Y.')\n",
    "    elif len(Y.shape)<2:\n",
    "        Y_dt_out, slopes, intercepts = linear_detrend(X,Y,remove_mn=remove_mn)\n",
    "        \n",
    "        return Y_dt_out, slopes, intercepts \n",
    "    else: \n",
    "        Y_dt_out = np.zeros_like(Y)\n",
    "        slopes = np.zeros(Y.shape[0])\n",
    "        intercepts = np.zeros(Y.shape[0])\n",
    "        \n",
    "        if axis==1: \n",
    "            dt_axis=0\n",
    "        else: \n",
    "            dt_axis=1\n",
    "\n",
    "        for i in range(Y.shape[dt_axis]):\n",
    "            Y_dt_out[i,:], slopes[i], intercepts[i] = linear_detrend(X,Y[i,:],remove_mn=remove_mn)\n",
    "            \n",
    "        return Y_dt_out, slopes, intercepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_dt_out, slope, intercept = multi_linear_detrend(timei,X_var,axis=1,remove_mn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_dict[var]['time'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_nans_mask = np.where(np.isnan(X_var),np.nan,1)\n",
    "Y = np.where(np.isnan(X_var),0,X_var)\n",
    "X = np.arange(0,var_dict[var]['time'].shape[0],1)\n",
    "[var_dt,slopes,intercepts] = multi_linear_detrend(X,Y,axis=1,remove_mn=False)\n",
    "x_var_dt = var_dt*var_nans_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 1060\n",
    "\n",
    "slopes[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(X_var[t,:])\n",
    "#plt.plot(X*slopes[t]+intercepts[t])\n",
    "#plt.plot(x_var_dt[t])\n",
    "plt.plot(X_var_dt[t])\n",
    "plt.xlim(11900,12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_var[t,:])\n",
    "plt.plot(X*slopes[t]+intercepts[t])\n",
    "plt.plot(x_var_dt[t])\n",
    "plt.plot(X_var_dt[t])\n",
    "plt.xlim(11900,12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(slopes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_var[0,:])\n",
    "plt.plot(timei*a+b)\n",
    "plt.plot(timei*gradient+intercept)\n",
    "plt.plot(Y_dt_out)\n",
    "plt.xlim(0,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_var[0,:])\n",
    "plt.plot(Y_dt_out)\n",
    "plt.xlim(0,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
